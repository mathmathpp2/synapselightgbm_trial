{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d5357-a45a-470e-bd07-c5f66297e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a5722-0545-47e7-a7ea-cd5d856b66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work\n",
    "# packages = [\n",
    "#     'org.apache.hadoop:hadoop-aws:3.4.0',\n",
    "#     'org.apache.hadoop:hadoop-client-api:3.4.0',\n",
    "#     'org.apache.hadoop:hadoop-client-runtime:3.4.0',\n",
    "# ]\n",
    "\n",
    "# works\n",
    "packages = [\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-api:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-runtime:3.3.4',\n",
    "]\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MyApp\") \\\n",
    "    .set(\"spark.driver.memory\", \"8g\") \\\n",
    "    .set(\"spark.executor.memory\", \"8g\") \\\n",
    "    .set('spark.jars.packages', ','.join(packages))\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# hadoop_config = spark._jsc.hadoopConfiguration()\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_config.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "hadoop_config.set('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "# hadoop_config.set('fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider')\n",
    "# hadoop_config.set('fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "# hadoop_config.set('fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)\n",
    "# hadoop_config.set('fs.s3a.session.token', AWS_SESSION_TOKEN)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173f746-1d1b-40f6-93ca-593da53750a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (0, \"red\", \"SUV\", 12, 20.0, 60, 1, \"2024-07-01\", \"u4pruydqqvj\"),\n",
    "    (1, \"red\", \"sedan\", 9, 30.0, 70, 2, \"2024-07-02\", \"u4pruydqqvk\"),\n",
    "    (2, \"red\", \"truck\", 15, 25.0, 80, 3, \"2024-07-01\", \"u4pruydqqvj\"),\n",
    "    (3, \"blue\", \"SUV\", 20, 22.0, 65, 1, \"2024-07-02\", \"u4pruydqqvk\"),\n",
    "    (4, \"blue\", \"sedan\", 5, 35.0, 75, 1, \"2024-07-01\", \"u4pruydqqvj\"),\n",
    "    (5, \"blue\", \"truck\", 10, 28.0, 85, 3, \"2024-07-02\", \"u4pruydqqvk\")\n",
    "], [\"id\", \"color\", \"type\", \"hour\", \"milesperhour\", \"age\", \"label\", \"date\", \"geohash\"])\n",
    "\n",
    "# Convert string labels to numeric\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "data = label_indexer.fit(data).transform(data)\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# String Indexing for features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for Logistic Regression (not needed for tree-based models)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\"],\n",
    "    outputCols=[\"color_vec\", \"type_vec\", \"hour_vec\"]\n",
    ")\n",
    "\n",
    "# Assembling Features for Logistic Regression and Naive Bayes\n",
    "assembler_lr_nb = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"type_vec\", \"hour_vec\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Assembling Features for Tree-Based Models\n",
    "assembler_tree = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create and Fit the Pipeline for Logistic Regression and Naive Bayes\n",
    "pipeline_lr_nb = Pipeline(stages=indexers + [encoder, assembler_lr_nb])\n",
    "model_lr_nb = pipeline_lr_nb.fit(train_data)\n",
    "transformed_train_data_lr_nb = model_lr_nb.transform(train_data)\n",
    "transformed_test_data_lr_nb = model_lr_nb.transform(test_data)\n",
    "\n",
    "# Create and Fit the Pipeline for Tree-Based Models\n",
    "pipeline_tree = Pipeline(stages=indexers + [assembler_tree])\n",
    "model_tree = pipeline_tree.fit(train_data)\n",
    "transformed_train_data_tree = model_tree.transform(train_data)\n",
    "transformed_test_data_tree = model_tree.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020b2e6-e9cc-472b-aeed-7d479545214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パイプラインで処理した pyspark データフレームから、features の順番で特徴量を取り出す\n",
    "def get_features(df):\n",
    "    feature_attrs = df.schema['features'].metadata['ml_attr']['attrs']\n",
    "    features = []\n",
    "    for attr_type, attrs in feature_attrs.items():\n",
    "        features += attrs\n",
    "\n",
    "    for each in sorted(features, key=lambda x: x['idx']):\n",
    "        print(each['idx'], each['name'])\n",
    "    \n",
    "    feature_names = [each['name'] for each in sorted(features, key=lambda x: x['idx'])]\n",
    "\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a003e5-eca8-457c-96ad-5fb7fe1bf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52f22d-1ba7-4361-b633-126ecc7ef110",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f377a-b2af-4af0-aa2f-b82a41f7138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(transformed_train_data_lr_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f9b1f-3d54-41f2-b6e6-71b445b3cfee",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef839c7b-5ce8-4a9c-8010-ab6aa38ae891",
   "metadata": {},
   "source": [
    "## Save and load (Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc9b5e-5e05-4b19-8041-8c41437ed57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "# data_path = \"data/transformed_train_data_lr_nb/\"\n",
    "\n",
    "# save to s3\n",
    "data_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/data/transformed_train_data_lr_nb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defbede-c20a-4e91-8b71-7f245bbec36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb.write.partitionBy(\"date\", \"geohash\").mode('overwrite').save(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f6047-b8d3-453d-a99f-84d1b0db6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb_loaded = spark.read.load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ce946-e0fc-479a-a6b6-a8ced26c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb_loaded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2ffaa-2662-454e-9b2e-9948a5b8e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(transformed_train_data_lr_nb_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02529b-a62d-4de7-9a02-2c511e1ffdd8",
   "metadata": {},
   "source": [
    "## Save and load (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e84567-e6e4-4974-b953-b4be8f7edfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline_lr_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c74f96-8ef5-4dda-995b-b2ef2888cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_lr_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a7b8f-1ecf-40b7-880c-f9a1bcf7575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "pipeline_model_path = \"pipelines/model_lr_nb\"\n",
    "model_lr_nb.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6aaca-3ea9-45a6-aeb5-465041ca15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to s3\n",
    "pipeline_model_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/pipelines/model_lr_nb\"\n",
    "model_lr_nb.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84677c0-d535-40ca-8aab-73a7ea731c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PipelineModel.load(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937bd2f-bc1d-4864-aff1-b587bd5bb123",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb_loaded = loaded_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466df539-ed76-4460-bb6f-5faab410b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_lr_nb_loaded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ec047-4752-464a-b6ea-eb93d6c5ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(transformed_train_data_lr_nb_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb015b-97f1-4b01-b0f2-e1395b57c35f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3531c-cca9-4321-8c0b-f00cb5793f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_index\")\n",
    "lr_model = lr.fit(transformed_train_data_lr_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303b02b-be49-488a-bb47-d454c42a0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# これはたぶんエラーになるのでかわりに次のセルを実行\n",
    "\n",
    "# # Get model coefficients and intercept for Logistic Regression\n",
    "# coefficients = lr_model.coefficients\n",
    "# intercept = lr_model.intercept\n",
    "# print(f\"Coefficients: {coefficients}\")\n",
    "# print(f\"Intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b0e56-a5aa-415b-af35-072c197945d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model coefficients and intercept for Logistic Regression\n",
    "coefficients = lr_model.coefficientMatrix\n",
    "intercept = lr_model.interceptVector\n",
    "print(f\"Coefficients: {coefficients}\")\n",
    "print(f\"Intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b2793-848e-4dd7-afcc-62669c17caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = num_classes x num_features\n",
    "np.array(coefficients.toArray().tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422423dc-197b-4ea2-9d14-42529b5aaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(\n",
    "    np.array(coefficients.toArray().tolist()),\n",
    "    columns=get_features(transformed_train_data_lr_nb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10890487-8afd-445e-b87f-12788ec686b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99885ab3-0428-44c6-9f71-e7501c9ddb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスと label の対応\n",
    "data.toPandas()[['label', 'label_index']].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef4f13-b7e8-4170-b9d1-b003c1db594a",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593d61e-e924-4ee0-ab20-77f0d4dd9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_tree.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58488ad9-844a-48a1-9d86-8bd4c63a837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(transformed_train_data_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adcc3c8-6f05-41e8-8ad8-145e5b75b3db",
   "metadata": {},
   "source": [
    "## Save and load (Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf93b27-fe3e-46d4-9319-911a4f23554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "# data_path = \"data/transformed_train_data_tree/\"\n",
    "\n",
    "# save to s3\n",
    "data_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/data/transformed_train_data_tree/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d8429-db42-4e16-9ea6-24746c84d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_tree.write.partitionBy(\"date\", \"geohash\").mode('overwrite').save(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468771a5-935f-44c8-b0ae-8b08192fabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_tree_loaded = spark.read.load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d987d6f-ab23-4886-8374-f3d260702120",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_tree_loaded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6b5f3-de83-4ce7-9d6f-c281d50547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(transformed_train_data_tree_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1748c-58c8-407c-be71-27e7898889ae",
   "metadata": {},
   "source": [
    "## Save and load (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16263-cd80-4130-b758-fc9c2e026e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "pipeline_model_path = \"pipelines/model_tree\"\n",
    "model_tree.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f239d8b-7810-4231-9738-ca6f626cd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to s3\n",
    "pipeline_model_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/pipelines/model_tree\"\n",
    "model_tree.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44a5e6-2e5b-4ad5-8084-22eb26a24d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PipelineModel.load(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e85b55-7003-4ae5-a25f-e9093f3e8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_tree_loaded = loaded_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399950a-90ab-4bb1-841b-cb6db2b7d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data_tree_loaded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b13936-96bc-41e5-a076-52c5c1375447",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(transformed_train_data_tree_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a441659-18ae-4a43-a0ce-c68dc2c9d5df",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d809de-ac10-4f03-9f1b-12945e87bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Get model summary to extract training metrics for Logistic Regression\n",
    "# training_summary = lr_model.summary\n",
    "# print(f\"Training Accuracy: {training_summary.accuracy}\")\n",
    "# print(f\"Training Precision: {training_summary.precisionByLabel}\")\n",
    "# print(f\"Training Recall: {training_summary.recallByLabel}\")\n",
    "# print(f\"Training F1 Score: {training_summary.fMeasureByLabel()}\")\n",
    "\n",
    "# Training the Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label_index\")\n",
    "rf_model = rf.fit(transformed_train_data_tree)\n",
    "\n",
    "# # Training the GBT Classifier\n",
    "# gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label_index\")\n",
    "# gbt_model = gbt.fit(transformed_train_data_tree)\n",
    "\n",
    "# # Training the Naive Bayes Classifier\n",
    "# nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label_index\")\n",
    "# nb_model = nb.fit(transformed_train_data_lr_nb)\n",
    "\n",
    "# # Initialize evaluators for all models\n",
    "# evaluator_accuracy = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"accuracy\")\n",
    "# evaluator_precision = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"weightedPrecision\")\n",
    "# evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"weightedRecall\")\n",
    "# evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"f1\")\n",
    "\n",
    "# # Evaluating the Logistic Regression Model\n",
    "# lr_predictions = lr_model.transform(transformed_test_data_lr_nb)\n",
    "# lr_accuracy = evaluator_accuracy.evaluate(lr_predictions)\n",
    "# lr_precision = evaluator_precision.evaluate(lr_predictions)\n",
    "# lr_recall = evaluator_recall.evaluate(lr_predictions)\n",
    "# lr_f1 = evaluator_f1.evaluate(lr_predictions)\n",
    "# print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
    "# print(f\"Logistic Regression Precision: {lr_precision}\")\n",
    "# print(f\"Logistic Regression Recall: {lr_recall}\")\n",
    "# print(f\"Logistic Regression F1 Score: {lr_f1}\")\n",
    "\n",
    "# # Evaluating the Random Forest Classifier\n",
    "# rf_predictions = rf_model.transform(transformed_test_data_tree)\n",
    "# rf_accuracy = evaluator_accuracy.evaluate(rf_predictions)\n",
    "# rf_precision = evaluator_precision.evaluate(rf_predictions)\n",
    "# rf_recall = evaluator_recall.evaluate(rf_predictions)\n",
    "# rf_f1 = evaluator_f1.evaluate(rf_predictions)\n",
    "# print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "# print(f\"Random Forest Precision: {rf_precision}\")\n",
    "# print(f\"Random Forest Recall: {rf_recall}\")\n",
    "# print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "\n",
    "# Get feature importances for Random Forest\n",
    "rf_feature_importances = rf_model.featureImportances.toArray()\n",
    "features_importances_rf = [(assembler_tree.getInputCols()[i], float(rf_feature_importances[i])) for i in range(len(rf_feature_importances))]\n",
    "importances_df_rf = pd.DataFrame(features_importances_rf, columns=[\"Feature\", \"Importance\"]).sort_values(by='Importance', ascending=False)\n",
    "print(importances_df_rf)\n",
    "\n",
    "# # Evaluating the GBT Classifier\n",
    "# gbt_predictions = gbt_model.transform(transformed_test_data_tree)\n",
    "# gbt_accuracy = evaluator_accuracy.evaluate(gbt_predictions)\n",
    "# gbt_precision = evaluator_precision.evaluate(gbt_predictions)\n",
    "# gbt_recall = evaluator_recall.evaluate(gbt_predictions)\n",
    "# gbt_f1 = evaluator_f1.evaluate(gbt_predictions)\n",
    "# print(f\"GBT Classifier Accuracy: {gbt_accuracy}\")\n",
    "# print(f\"GBT Classifier Precision: {gbt_precision}\")\n",
    "# print(f\"GBT Classifier Recall: {gbt_recall}\")\n",
    "# print(f\"GBT Classifier F1 Score: {gbt_f1}\")\n",
    "\n",
    "# # Get feature importances for GBT\n",
    "# gbt_feature_importances = gbt_model.featureImportances.toArray()\n",
    "# features_importances_gbt = [(assembler_tree.getInputCols()[i], float(gbt_feature_importances[i])) for i in range(len(gbt_feature_importances))]\n",
    "# importances_df_gbt = pd.DataFrame(features_importances_gbt, columns=[\"Feature\", \"Importance\"]).sort_values(by='Importance', ascending=False)\n",
    "# print(importances_df_gbt)\n",
    "\n",
    "# # Evaluating the Naive Bayes Classifier\n",
    "# nb_predictions = nb_model.transform(transformed_test_data_lr_nb)\n",
    "# nb_accuracy = evaluator_accuracy.evaluate(nb_predictions)\n",
    "# nb_precision = evaluator_precision.evaluate(nb_predictions)\n",
    "# nb_recall = evaluator_recall.evaluate(nb_predictions)\n",
    "# nb_f1 = evaluator_f1.evaluate(nb_predictions)\n",
    "# print(f\"Naive Bayes Accuracy: {nb_accuracy}\")\n",
    "# print(f\"Naive Bayes Precision: {nb_precision}\")\n",
    "# print(f\"Naive Bayes Recall: {nb_recall}\")\n",
    "# print(f\"Naive Bayes F1 Score: {nb_f1}\")\n",
    "\n",
    "# # Naive Bayes Model Parameters\n",
    "# print(f\"Naive Bayes Model Parameters: {nb_model.explainParams()}\")\n",
    "\n",
    "# # Extract and print class prior probabilities and conditional probabilities for Naive Bayes\n",
    "# class_prior_probs = np.exp(nb_model.pi.toArray())\n",
    "# conditional_probs = np.exp(nb_model.theta.toArray())\n",
    "\n",
    "# # Get the number of classes and features\n",
    "# num_classes, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f0eb0-df6f-4e0e-9872-bac946841536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
