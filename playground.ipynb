{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics \n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RegressionModelEvaluation\").getOrCreate()\n",
    "\n",
    "# Load your data\n",
    "data = spark.read.format(\"libsvm\").load(\"path/to/your/data\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define the models\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label')\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='label')\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Train the models\n",
    "lr_model = lr.fit(train_data)\n",
    "rf_model = rf.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Initialize evaluators\n",
    "r2_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"r2\")\n",
    "rmse_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "mae_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"mae\")\n",
    "mse_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"mse\")\n",
    "\n",
    "# Evaluate the models\n",
    "models = {\n",
    "    \"Linear Regression\": lr_predictions,\n",
    "    \"Random Forest Regressor\": rf_predictions,\n",
    "    \"GBT Regressor\": gbt_predictions\n",
    "}\n",
    "\n",
    "for name, predictions in models.items():\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "    rmse = rmse_evaluator.evaluate(predictions)\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    mse = mse_evaluator.evaluate(predictions)\n",
    "    print(f\"{name} Evaluation Metrics:\")\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"GBTRegressorExample\").getOrCreate()\n",
    "\n",
    "# Create the DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (0, \"red\", \"SUV\", 12, 20.0, 60, 5),\n",
    "    (1, \"blue\", \"sedan\", 9, 30.0, 70, 10),\n",
    "    (2, \"green\", \"truck\", 15, 25.0, 80, 3)\n",
    "], [\"id\", \"color\", \"type\", \"hour\", \"label\", \"milesperhour\", \"age\"])\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# Assembling Features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create and Fit the Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "model = pipeline.fit(data)\n",
    "transformed_data = model.transform(data)\n",
    "\n",
    "# Training the GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt_model = gbt.fit(transformed_data)\n",
    "\n",
    "# View Transformed Data (Optional)\n",
    "transformed_data.select(\"id\", \"features\", \"label\").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RegressorExamples\").getOrCreate()\n",
    "\n",
    "# Create the DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (0, \"red\", \"SUV\", 12, 20.0, 60, 5),\n",
    "    (1, \"blue\", \"sedan\", 9, 30.0, 70, 10),\n",
    "    (2, \"green\", \"truck\", 15, 25.0, 80, 3)\n",
    "], [\"id\", \"color\", \"type\", \"hour\", \"label\", \"milesperhour\", \"age\"])\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for Linear Regression (not needed for tree-based models)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\"],\n",
    "    outputCols=[\"color_vec\", \"type_vec\", \"hour_vec\"]\n",
    ")\n",
    "\n",
    "# Assembling Features for Linear Regression\n",
    "assembler_lr = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"type_vec\", \"hour_vec\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Assembling Features for Tree-Based Models\n",
    "assembler_tree = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create and Fit the Pipeline for Linear Regression\n",
    "pipeline_lr = Pipeline(stages=indexers + [encoder, assembler_lr])\n",
    "model_lr = pipeline_lr.fit(data)\n",
    "transformed_data_lr = model_lr.transform(data)\n",
    "\n",
    "# Create and Fit the Pipeline for Tree-Based Models\n",
    "pipeline_tree = Pipeline(stages=indexers + [assembler_tree])\n",
    "model_tree = pipeline_tree.fit(data)\n",
    "transformed_data_tree = model_tree.transform(data)\n",
    "\n",
    "# Training the Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(transformed_data_lr)\n",
    "\n",
    "# Training the Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(transformed_data_tree)\n",
    "\n",
    "# Training the GBT Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt_model = gbt.fit(transformed_data_tree)\n",
    "\n",
    "# View Transformed Data (Optional)\n",
    "transformed_data_lr.select(\"id\", \"features\", \"label\").show()\n",
    "transformed_data_tree.select(\"id\", \"features\", \"label\").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RegressorExamples\").getOrCreate()\n",
    "\n",
    "# Create the DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (0, \"red\", \"SUV\", 12, 20.0, 60, 5),\n",
    "    (1, \"blue\", \"sedan\", 9, 30.0, 70, 10),\n",
    "    (2, \"green\", \"truck\", 15, 25.0, 80, 3),\n",
    "    (3, \"yellow\", \"SUV\", 20, 22.0, 65, 6),\n",
    "    (4, \"white\", \"sedan\", 5, 35.0, 75, 12),\n",
    "    (5, \"black\", \"truck\", 10, 28.0, 85, 7)\n",
    "], [\"id\", \"color\", \"type\", \"hour\", \"label\", \"milesperhour\", \"age\"])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for Linear Regression (not needed for tree-based models)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\"],\n",
    "    outputCols=[\"color_vec\", \"type_vec\", \"hour_vec\"]\n",
    ")\n",
    "\n",
    "# Assembling Features for Linear Regression\n",
    "assembler_lr = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"type_vec\", \"hour_vec\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Assembling Features for Tree-Based Models\n",
    "assembler_tree = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create and Fit the Pipeline for Linear Regression\n",
    "pipeline_lr = Pipeline(stages=indexers + [encoder, assembler_lr])\n",
    "model_lr = pipeline_lr.fit(train_data)\n",
    "transformed_train_data_lr = model_lr.transform(train_data)\n",
    "transformed_test_data_lr = model_lr.transform(test_data)\n",
    "\n",
    "# Create and Fit the Pipeline for Tree-Based Models\n",
    "pipeline_tree = Pipeline(stages=indexers + [assembler_tree])\n",
    "model_tree = pipeline_tree.fit(train_data)\n",
    "transformed_train_data_tree = model_tree.transform(train_data)\n",
    "transformed_test_data_tree = model_tree.transform(test_data)\n",
    "\n",
    "# Training the Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(transformed_train_data_lr)\n",
    "\n",
    "# Training the Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(transformed_train_data_tree)\n",
    "\n",
    "# Training the GBT Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt_model = gbt.fit(transformed_train_data_tree)\n",
    "\n",
    "# Evaluating the Linear Regression Model\n",
    "lr_predictions = lr_model.transform(transformed_test_data_lr)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "lr_rmse = lr_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "\n",
    "# Evaluating the Random Forest Regressor\n",
    "rf_predictions = rf_model.transform(transformed_test_data_tree)\n",
    "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "\n",
    "# Evaluating the GBT Regressor\n",
    "gbt_predictions = gbt_model.transform(transformed_test_data_tree)\n",
    "gbt_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"GBT Regressor RMSE: {gbt_rmse}\")\n",
    "\n",
    "# View Transformed Data (Optional)\n",
    "transformed_test_data_lr.select(\"id\", \"features\", \"label\", \"prediction\").show()\n",
    "transformed_test_data_tree.select(\"id\", \"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null drop\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RegressorExamples\").getOrCreate()\n",
    "\n",
    "# Create the DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (0, \"red\", \"SUV\", 12, 20.0, 60, 5, 1),\n",
    "    (1, \"blue\", \"sedan\", 9, 30.0, 70, 10, 0),\n",
    "    (2, \"green\", \"truck\", 15, 25.0, 80, 3, 1),\n",
    "    (3, \"yellow\", \"SUV\", 20, 22.0, 65, 6, 0),\n",
    "    (4, \"white\", \"sedan\", 5, 35.0, 75, 12, 1),\n",
    "    (5, \"black\", \"truck\", 10, 28.0, 85, 7, 0),\n",
    "    (6, None, \"sedan\", 8, None, 70, 9, 1),  # Example with null values\n",
    "    (7, \"blue\", None, 5, 30.0, None, 4, 1)\n",
    "], [\"id\", \"color\", \"type\", \"hour\", \"label\", \"milesperhour\", \"age\", \"isnew\"])\n",
    "\n",
    "# Drop rows with any null values\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for Linear Regression (not needed for tree-based models)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\"],\n",
    "    outputCols=[\"color_vec\", \"type_vec\", \"hour_vec\"]\n",
    ")\n",
    "\n",
    "# Assembling Features for Linear Regression\n",
    "assembler_lr = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"type_vec\", \"hour_vec\", \"milesperhour\", \"age\", \"isnew\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Assembling Features for Tree-Based Models\n",
    "assembler_tree = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\", \"isnew\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create and Fit the Pipeline for Linear Regression\n",
    "pipeline_lr = Pipeline(stages=indexers + [encoder, assembler_lr])\n",
    "model_lr = pipeline_lr.fit(train_data)\n",
    "transformed_train_data_lr = model_lr.transform(train_data)\n",
    "transformed_test_data_lr = model_lr.transform(test_data)\n",
    "\n",
    "# Create and Fit the Pipeline for Tree-Based Models\n",
    "pipeline_tree = Pipeline(stages=indexers + [assembler_tree])\n",
    "model_tree = pipeline_tree.fit(train_data)\n",
    "transformed_train_data_tree = model_tree.transform(train_data)\n",
    "transformed_test_data_tree = model_tree.transform(test_data)\n",
    "\n",
    "# Training the Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(transformed_train_data_lr)\n",
    "\n",
    "# Training the Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(transformed_train_data_tree)\n",
    "\n",
    "# Training the GBT Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt_model = gbt.fit(transformed_train_data_tree)\n",
    "\n",
    "# Evaluating the Linear Regression Model\n",
    "lr_predictions = lr_model.transform(transformed_test_data_lr)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "lr_rmse = lr_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "\n",
    "# Evaluating the Random Forest Regressor\n",
    "rf_predictions = rf_model.transform(transformed_test_data_tree)\n",
    "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "\n",
    "# Evaluating the GBT Regressor\n",
    "gbt_predictions = gbt_model.transform(transformed_test_data_tree)\n",
    "gbt_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"GBT Regressor RMSE: {gbt_rmse}\")\n",
    "\n",
    "# View Transformed Data (Optional)\n",
    "transformed_test_data_lr.select(\"id\", \"features\", \"label\", \"prediction\").show()\n",
    "transformed_test_data_tree.select(\"id\", \"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RegressorExamples\").getOrCreate()\n",
    "\n",
    "# Create the DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (0, \"red\", \"SUV\", 12, 20.0, 60, 5, 1),\n",
    "    (1, \"blue\", \"sedan\", 9, 30.0, 70, 10, 0),\n",
    "    (2, \"green\", \"truck\", 15, 25.0, 80, 3, 1),\n",
    "    (3, \"yellow\", \"SUV\", 20, 22.0, 65, 6, 0),\n",
    "    (4, \"white\", \"sedan\", 5, 35.0, 75, 12, 1),\n",
    "    (5, \"black\", \"truck\", 10, 28.0, 85, 7, 0),\n",
    "    (6, None, \"sedan\", 8, None, 70, 9, 1),  # Example with null values\n",
    "    (7, \"blue\", None, 5, 30.0, None, 4, 1)\n",
    "], [\"id\", \"color\", \"type\", \"hour\", \"label\", \"milesperhour\", \"age\", \"isnew\"])\n",
    "\n",
    "# Drop rows with any null values\n",
    "data = data.dropna()\n",
    "\n",
    "# Debugging: Print schema and check for nulls in specific columns\n",
    "data.printSchema()\n",
    "data.select(\"color\", \"type\", \"hour\").show()\n",
    "\n",
    "# Fill nulls in categorical columns with 'missing'\n",
    "data = data.withColumn(\"color\", coalesce(data[\"color\"], lit(\"missing\")))\n",
    "data = data.withColumn(\"type\", coalesce(data[\"type\"], lit(\"missing\")))\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# String Indexing\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for Linear Regression (not needed for tree-based models)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\"],\n",
    "    outputCols=[\"color_vec\", \"type_vec\", \"hour_vec\"]\n",
    ")\n",
    "\n",
    "# Assembling Features for Linear Regression\n",
    "assembler_lr = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"type_vec\", \"hour_vec\", \"milesperhour\", \"age\", \"isnew\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Assembling Features for Tree-Based Models\n",
    "assembler_tree = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\", \"isnew\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create and Fit the Pipeline for Linear Regression\n",
    "pipeline_lr = Pipeline(stages=indexers + [encoder, assembler_lr])\n",
    "model_lr = pipeline_lr.fit(train_data)\n",
    "transformed_train_data_lr = model_lr.transform(train_data)\n",
    "transformed_test_data_lr = model_lr.transform(test_data)\n",
    "\n",
    "# Create and Fit the Pipeline for Tree-Based Models\n",
    "pipeline_tree = Pipeline(stages=indexers + [assembler_tree])\n",
    "model_tree = pipeline_tree.fit(train_data)\n",
    "transformed_train_data_tree = model_tree.transform(train_data)\n",
    "transformed_test_data_tree = model_tree.transform(test_data)\n",
    "\n",
    "# Training the Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(transformed_train_data_lr)\n",
    "\n",
    "# Training the Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(transformed_train_data_tree)\n",
    "\n",
    "# Training the GBT Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt_model = gbt.fit(transformed_train_data_tree)\n",
    "\n",
    "# Evaluating the Linear Regression Model\n",
    "lr_predictions = lr_model.transform(transformed_test_data_lr)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "lr_rmse = lr_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "\n",
    "# Evaluating the Random Forest Regressor\n",
    "rf_predictions = rf_model.transform(transformed_test_data_tree)\n",
    "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "\n",
    "# Evaluating the GBT Regressor\n",
    "gbt_predictions = gbt_model.transform(transformed_test_data_tree)\n",
    "gbt_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"GBT Regressor RMSE: {gbt_rmse}\")\n",
    "\n",
    "# View Transformed Data (Optional)\n",
    "transformed_test_data_lr.select(\"id\", \"features\", \"label\", \"prediction\").show()\n",
    "transformed_test_data_tree.select(\"id\", \"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"HistogramExample\").getOrCreate()\n",
    "\n",
    "# Assuming you have a DataFrame df with an 'error' column\n",
    "# df = spark.read.csv('path_to_your_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Convert the 'error' column to a list of values\n",
    "error_values = df.select(col('error')).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create the histogram data using Matplotlib\n",
    "plt.hist(error_values, bins=50, edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Error')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, floor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"HistogramExample\").getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (replace with your actual DataFrame)\n",
    "data = [\n",
    "    (1, None, 3.0, float('nan'), 'abc', True),\n",
    "    (2, 2, None, 4.0, None, False),\n",
    "    (None, 3, 3.5, None, 'def', None),\n",
    "    (4, 4, 4.5, 5.0, 'ghi', True)\n",
    "]\n",
    "\n",
    "columns = ['int_col', 'float_col', 'double_col', 'nan_col', 'string_col', 'bool_col']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Select the column to create the histogram for (e.g., 'float_col')\n",
    "column_to_plot = 'float_col'\n",
    "\n",
    "# Number of bins for the histogram\n",
    "num_bins = 10\n",
    "\n",
    "# Create bins and count the number of values in each bin\n",
    "min_val = df.agg({column_to_plot: 'min'}).collect()[0][0]\n",
    "max_val = df.agg({column_to_plot: 'max'}).collect()[0][0]\n",
    "bin_width = (max_val - min_val) / num_bins\n",
    "\n",
    "# Calculate the bin edges\n",
    "bins = [min_val + i * bin_width for i in range(num_bins + 1)]\n",
    "\n",
    "# Bin the data\n",
    "binned_df = df.withColumn('bin', floor((col(column_to_plot) - min_val) / bin_width))\n",
    "\n",
    "# Count the number of entries in each bin\n",
    "histogram_df = binned_df.groupBy('bin').agg(count('*').alias('count')).orderBy('bin')\n",
    "\n",
    "# Collect the histogram data\n",
    "histogram_data = histogram_df.collect()\n",
    "\n",
    "# Extract bin labels and counts\n",
    "bin_labels = [min_val + i * bin_width for i in range(num_bins)]\n",
    "counts = [row['count'] for row in histogram_data]\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.bar(bin_labels, counts, width=bin_width, edgecolor='black', align='edge')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(column_to_plot)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Histogram of {column_to_plot}')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, floor, when\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"HistogramWithOutliersExample\").getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (replace with your actual DataFrame)\n",
    "data = [\n",
    "    (1, None, 3.0, float('nan'), 'abc', True),\n",
    "    (2, 2, None, 4.0, None, False),\n",
    "    (None, 3, 3.5, None, 'def', None),\n",
    "    (4, 4, 4.5, 50.0, 'ghi', True)\n",
    "]\n",
    "\n",
    "columns = ['int_col', 'float_col', 'double_col', 'nan_col', 'string_col', 'bool_col']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Select the column to create the histogram for (e.g., 'double_col')\n",
    "column_to_plot = 'double_col'\n",
    "\n",
    "# Number of bins for the histogram\n",
    "num_bins = 10\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "min_val = df.agg({column_to_plot: 'min'}).collect()[0][0]\n",
    "max_val = df.agg({column_to_plot: 'max'}).collect()[0][0]\n",
    "\n",
    "# Define the threshold for outliers\n",
    "threshold = max_val - (max_val - min_val) * 0.1  # Example threshold at 90% of the max value\n",
    "\n",
    "# Calculate the bin width\n",
    "bin_width = (threshold - min_val) / num_bins\n",
    "\n",
    "# Create bins and group outliers\n",
    "binned_df = df.withColumn(\n",
    "    'bin',\n",
    "    when(col(column_to_plot) > threshold, num_bins)\n",
    "    .otherwise(floor((col(column_to_plot) - min_val) / bin_width))\n",
    ")\n",
    "\n",
    "# Count the number of entries in each bin\n",
    "histogram_df = binned_df.groupBy('bin').agg(count('*').alias('count')).orderBy('bin')\n",
    "\n",
    "# Collect the histogram data\n",
    "histogram_data = histogram_df.collect()\n",
    "\n",
    "# Extract bin labels and counts\n",
    "bin_labels = [min_val + i * bin_width for i in range(num_bins)] + ['Outliers']\n",
    "counts = [0] * (num_bins + 1)\n",
    "for row in histogram_data:\n",
    "    bin_index = row['bin']\n",
    "    counts[bin_index] = row['count']\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.bar(bin_labels[:-1], counts[:-1], width=bin_width, edgecolor='black', align='edge', label='Data')\n",
    "plt.bar(bin_labels[-1], counts[-1], width=bin_width, edgecolor='black', align='edge', label='Outliers')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(column_to_plot)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Histogram of {column_to_plot}')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error mismatch shape fox?\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, floor, when, lit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"HistogramWithOutliersExample\").getOrCreate()\n",
    "\n",
    "# Example DataFrame creation (replace with your actual DataFrame)\n",
    "data = [\n",
    "    (1, None, 3.0, float('nan'), 'abc', True),\n",
    "    (2, 2, None, 4.0, None, False),\n",
    "    (None, 3, 3.5, None, 'def', None),\n",
    "    (4, 4, 4.5, 50.0, 'ghi', True)\n",
    "]\n",
    "\n",
    "columns = ['int_col', 'float_col', 'double_col', 'nan_col', 'string_col', 'bool_col']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Select the column to create the histogram for (e.g., 'double_col')\n",
    "column_to_plot = 'double_col'\n",
    "\n",
    "# Number of bins for the histogram\n",
    "num_bins = 10\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "min_val = df.agg({column_to_plot: 'min'}).collect()[0][0]\n",
    "max_val = df.agg({column_to_plot: 'max'}).collect()[0][0]\n",
    "\n",
    "# Define the threshold for outliers\n",
    "threshold = max_val - (max_val - min_val) * 0.1  # Example threshold at 90% of the max value\n",
    "\n",
    "# Calculate the bin width\n",
    "bin_width = (threshold - min_val) / num_bins\n",
    "\n",
    "# Create bins and group outliers\n",
    "binned_df = df.withColumn(\n",
    "    'bin',\n",
    "    when(col(column_to_plot) > threshold, num_bins)\n",
    "    .otherwise(floor((col(column_to_plot) - min_val) / bin_width))\n",
    ")\n",
    "\n",
    "# Count the number of entries in each bin\n",
    "histogram_df = binned_df.groupBy('bin').agg(count('*').alias('count')).orderBy('bin')\n",
    "\n",
    "# Collect the histogram data\n",
    "histogram_data = histogram_df.collect()\n",
    "\n",
    "# Extract bin labels and counts\n",
    "bin_labels = [min_val + i * bin_width for i in range(num_bins)] + ['Outliers']\n",
    "counts = [0] * (num_bins + 1)\n",
    "for row in histogram_data:\n",
    "    bin_index = int(row['bin']) if row['bin'] != 'Outliers' else num_bins\n",
    "    counts[bin_index] = row['count']\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.bar(range(num_bins), counts[:-1], width=bin_width, edgecolor='black', align='edge', label='Data')\n",
    "plt.bar(num_bins, counts[-1], width=bin_width, edgecolor='black', align='edge', label='Outliers')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xticks(range(num_bins + 1), bin_labels, rotation=45)\n",
    "plt.xlabel(column_to_plot)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Histogram of {column_to_plot}')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
