{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef940182-164e-47a3-99f6-e9f5421951ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import expr, col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "packages = [\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-api:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-runtime:3.3.4',\n",
    "    'io.delta:delta-core_2.12:2.4.0',\n",
    "]\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MyApp\") \\\n",
    "    .set(\"spark.driver.memory\", \"8g\") \\\n",
    "    .set(\"spark.executor.memory\", \"8g\") \\\n",
    "    .set('spark.jars.packages', ','.join(packages)) \\\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "hadoop_config.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "hadoop_config.set('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b390121-f69c-49c2-87a6-4a6fd264b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitionProcessor:\n",
    "    def __init__(self, group_by_cols, agg_dict, date_col=\"ds\", partition_col=\"partitionArea\", median_accuracy=100, variance_default=0.0):\n",
    "        self.date_col = date_col\n",
    "        self.partition_col = partition_col\n",
    "        self.group_by_cols = group_by_cols\n",
    "        self._group_by_cols = [self.date_col, self.partition_col] + self.group_by_cols\n",
    "        self.agg_dict = agg_dict\n",
    "        self.median_accuracy = median_accuracy\n",
    "        self.variance_default = variance_default\n",
    "\n",
    "        self._agg_exprs = self._get_aggregation_expressions()\n",
    "    \n",
    "    def _get_aggregation_expressions(self):\n",
    "        agg_exprs = []\n",
    "        for col_name, agg_funcs in self.agg_dict.items():\n",
    "            for func in agg_funcs:\n",
    "                if func == 'variance':\n",
    "                    agg_exprs.append(F.var_pop(col_name).alias(f\"{col_name}_variance\"))\n",
    "                elif func == 'count':\n",
    "                    agg_exprs.append(F.count(col_name).alias(f\"{col_name}_count\"))\n",
    "                elif func == 'sum':\n",
    "                    agg_exprs.append(F.sum(col_name).alias(f\"{col_name}_sum\"))\n",
    "                elif func == 'avg':\n",
    "                    agg_exprs.append(F.avg(col_name).alias(f\"{col_name}_avg\"))\n",
    "                elif func == 'median':\n",
    "                    agg_exprs.append(F.expr(f'percentile_approx({col_name}, 0.5, {self.median_accuracy})').alias(f\"{col_name}_median\"))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported aggregation function: {func}\")\n",
    "        return agg_exprs\n",
    "\n",
    "    def group_and_aggregate(self, df):\n",
    "        result_df = df.groupBy(self._group_by_cols).agg(*self._agg_exprs)\n",
    "        \n",
    "        for col_name, agg_funcs in self.agg_dict.items():\n",
    "            if 'variance' in agg_funcs:\n",
    "                result_df = result_df.fillna({f\"{col_name}_variance\": self.variance_default})\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def process_dates(self, dates, table_name, num_partitions=200):\n",
    "        for date in dates:\n",
    "            print(date)\n",
    "\n",
    "            df_date = df.filter(col(self.date_col) == date)\n",
    "            df_date_repart = df_date.repartition(num_partitions, self._group_by_cols)\n",
    "            result_df = self.group_and_aggregate(df_date_repart)\n",
    "\n",
    "            (\n",
    "                result_df\n",
    "                # .repartition(num_partitions)\n",
    "                .write\n",
    "                .partitionBy(self.date_col, self.partition_col)\n",
    "                .format(\"delta\")\n",
    "                .mode('overwrite')\n",
    "                .option('replaceWhere', f\"{self.date_col} = '{date}'\")\n",
    "                .option('mergeSchema', 'true')\n",
    "                .saveAsTable(table_name)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a972a641-1cee-4215-8e93-ac1654cb6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# initial data\n",
    "data = [\n",
    "    (\"2021-07-30\", \"partition1\", \"A\", \"X\", 10),\n",
    "    (\"2021-07-30\", \"partition1\", \"A\", \"X\", 20),\n",
    "    (\"2021-07-30\", \"partition2\", \"A\", \"Y\", 30),\n",
    "    (\"2021-07-31\", \"partition1\", \"B\", \"X\", 40),\n",
    "    (\"2021-07-31\", \"partition2\", \"B\", \"Y\", 50),\n",
    "    (\"2021-07-31\", \"partition2\", \"C\", \"Y\", None),  # Example with a null value\n",
    "]\n",
    "\n",
    "# additional data\n",
    "# data = [\n",
    "#     (\"2021-08-01\", \"partition1\", \"D\", \"X\", 20),\n",
    "#     (\"2021-08-01\", \"partition2\", \"A\", \"Z\", 10),\n",
    "#     (\"2021-08-01\", \"partition2\", \"A\", \"Y\", 30),\n",
    "# ]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"ds\", \"partitionArea\", \"col1\", \"col2\", \"value\"])\n",
    "df = df.withColumn(\"ds\", df[\"ds\"].cast(DateType()))\n",
    "\n",
    "group_by_cols = [\"col1\", \"col2\"]\n",
    "agg_dict = {\n",
    "    \"value\": [\"variance\", \"count\", \"sum\", \"avg\", \"median\"]\n",
    "}\n",
    "\n",
    "table_name = 'test'\n",
    "processor = PartitionProcessor(group_by_cols, agg_dict)\n",
    "\n",
    "# initial data\n",
    "dates = [\"2021-07-30\", \"2021-07-31\"]\n",
    "\n",
    "# dates to append to the same table\n",
    "# dates = [\"2021-08-01\"]\n",
    "\n",
    "processor.process_dates(dates=dates, table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1c52209-66cc-49f4-8cb6-ca0be671f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thama/ghq/github.com/mathmathpp2/synapselightgbm_trial/synapse-test/.venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-07-28 05:04:32.824</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '3', 'numRemovedBytes': '0',...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.4.3 Delta-Lake/2.4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-07-28 04:59:20.984</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '3', 'numRemovedBytes': '585...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.4.3 Delta-Lake/2.4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-07-28 04:59:16.903</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '2', 'numRemovedBytes': '402...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.4.3 Delta-Lake/2.4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-28 04:56:55.637</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '3', 'numRemovedBytes': '0',...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.4.3 Delta-Lake/2.4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-07-28 04:56:48.739</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '2', 'numAddedChangeFiles': ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.4.3 Delta-Lake/2.4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation  \\\n",
       "0        4 2024-07-28 05:04:32.824   None     None     WRITE   \n",
       "1        3 2024-07-28 04:59:20.984   None     None     WRITE   \n",
       "2        2 2024-07-28 04:59:16.903   None     None     WRITE   \n",
       "3        1 2024-07-28 04:56:55.637   None     None     WRITE   \n",
       "4        0 2024-07-28 04:56:48.739   None     None     WRITE   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...  None     None      None   \n",
       "1  {'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...  None     None      None   \n",
       "2  {'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...  None     None      None   \n",
       "3  {'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...  None     None      None   \n",
       "4  {'mode': 'Overwrite', 'partitionBy': '[\"ds\",\"p...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          3.0   Serializable          False   \n",
       "1          2.0   Serializable          False   \n",
       "2          1.0   Serializable          False   \n",
       "3          0.0   Serializable          False   \n",
       "4          NaN   Serializable           True   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '3', 'numRemovedBytes': '0',...         None   \n",
       "1  {'numOutputRows': '3', 'numRemovedBytes': '585...         None   \n",
       "2  {'numOutputRows': '2', 'numRemovedBytes': '402...         None   \n",
       "3  {'numOutputRows': '3', 'numRemovedBytes': '0',...         None   \n",
       "4  {'numOutputRows': '2', 'numAddedChangeFiles': ...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.4.3 Delta-Lake/2.4.0  \n",
       "1  Apache-Spark/3.4.3 Delta-Lake/2.4.0  \n",
       "2  Apache-Spark/3.4.3 Delta-Lake/2.4.0  \n",
       "3  Apache-Spark/3.4.3 Delta-Lake/2.4.0  \n",
       "4  Apache-Spark/3.4.3 Delta-Lake/2.4.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, table_name)\n",
    "delta_table.history().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "137b84c6-3e01-4309-93f3-a447dd5881db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = spark.read.format(\"delta\").table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec909d45-12d1-41bb-9bc8-040275e5297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----+----+--------------+-----------+---------+---------+------------+\n",
      "|        ds|partitionArea|col1|col2|value_variance|value_count|value_sum|value_avg|value_median|\n",
      "+----------+-------------+----+----+--------------+-----------+---------+---------+------------+\n",
      "|2021-08-01|   partition1|   D|   X|           0.0|          1|       20|     20.0|          20|\n",
      "|2021-07-31|   partition1|   B|   X|           0.0|          1|       40|     40.0|          40|\n",
      "|2021-07-31|   partition2|   B|   Y|           0.0|          1|       50|     50.0|          50|\n",
      "|2021-08-01|   partition2|   A|   Y|           0.0|          1|       30|     30.0|          30|\n",
      "|2021-08-01|   partition2|   A|   Z|           0.0|          1|       10|     10.0|          10|\n",
      "|2021-07-30|   partition2|   A|   Y|           0.0|          1|       30|     30.0|          30|\n",
      "|2021-07-30|   partition1|   A|   X|          25.0|          2|       30|     15.0|          10|\n",
      "|2021-07-31|   partition2|   C|   Y|           0.0|          0|     null|     null|        null|\n",
      "+----------+-------------+----+----+--------------+-----------+---------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97961111-1751-4e09-84e0-b878b6394d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
