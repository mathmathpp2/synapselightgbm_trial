{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0d5357-a45a-470e-bd07-c5f66297e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit, rand\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegressionModel, RandomForestClassificationModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154a5722-0545-47e7-a7ea-cd5d856b66d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/20 02:26:29 WARN Utils: Your hostname, thamaMBP.local resolves to a loopback address: 127.0.0.1; using 172.20.10.5 instead (on interface en0)\n",
      "24/07/20 02:26:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/thama/ghq/github.com/mathmathpp2/synapselightgbm_trial/synapse-test/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/thama/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/thama/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client-api added as a dependency\n",
      "org.apache.hadoop#hadoop-client-runtime added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fcdcdc27-d263-49fa-a193-e03caf24d779;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      ":: resolution report :: resolve 496ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fcdcdc27-d263-49fa-a193-e03caf24d779\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/14ms)\n",
      "24/07/20 02:26:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "packages = [\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-api:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-runtime:3.3.4',\n",
    "]\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MyApp\") \\\n",
    "    .set(\"spark.driver.memory\", \"8g\") \\\n",
    "    .set(\"spark.executor.memory\", \"8g\") \\\n",
    "    .set('spark.jars.packages', ','.join(packages))\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_config = sc._jsc.hadoopConfiguration()\n",
    "hadoop_config.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "hadoop_config.set('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d153dbc-e686-4e2a-bc5c-27ab8de9ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10c2ebd10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7cbf26-cb8d-42c0-b471-a5d13497ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----+------------------+---+-----+----------+-----------+\n",
      "| id|color| type|hour|      milesperhour|age|label|      date|    geohash|\n",
      "+---+-----+-----+----+------------------+---+-----+----------+-----------+\n",
      "|  0| blue|sedan|   1|34.082181961217046| 48|    3|2024-07-01|24i48e9i4j1|\n",
      "|  1|  red|sedan|   0|23.838102366558587| 83|    1|2024-07-02|7tkx93qn8z9|\n",
      "|  2| blue|truck|   2|32.368643573261394| 85|    1|2024-07-03|97sgui0463f|\n",
      "|  3| blue|truck|   4|23.554232985756123| 65|    3|2024-07-02|yb3w6c9km7w|\n",
      "|  4|green|  SUV|   3|27.497758222648912| 50|    3|2024-07-04|kcwv1lmmcrc|\n",
      "|  5| blue|  SUV|   4|28.390148083816293| 72|    2|2024-07-02|t758bkka16i|\n",
      "|  6|green|  SUV|   2| 30.62108528150685| 42|    3|2024-07-02|2h79c7blsjn|\n",
      "|  7|green|  SUV|   1|33.295822665972786| 81|    3|2024-07-02|euwjxkr18v6|\n",
      "|  8|  red|  SUV|   1|20.431989967799687| 63|    1|2024-07-04|kcig7tidkwo|\n",
      "|  9|  red|  SUV|   1| 19.58002103070639| 55|    1|2024-07-04|dblwpbpfrio|\n",
      "| 10|  red|truck|   4| 25.06522880234153| 84|    1|2024-07-01|lyfximkme6t|\n",
      "| 11|green|truck|   1|20.458736286970375| 80|    3|2024-07-02|ja8k8ufsx6z|\n",
      "| 12|  red|truck|   0|26.149960464469473| 47|    1|2024-07-04|jw644hf7ic5|\n",
      "| 13|  red|truck|   2| 23.76671139201894| 76|    1|2024-07-02|u5cc4fhsme9|\n",
      "| 14|  red|truck|   2| 24.37966922594645| 55|    1|2024-07-04|4t7p4vt92kb|\n",
      "| 15|  red|truck|   4|29.575660737257067| 41|    1|2024-07-02|ck3yqsuli48|\n",
      "| 16|  red|  SUV|   1| 30.79119559066366| 69|    1|2024-07-03|ou4e5yrrrhb|\n",
      "| 17|green|truck|   0|27.152573387642487| 40|    3|2024-07-01|hk96sjcwoo6|\n",
      "| 18|green|truck|   4|22.857467120913775| 50|    3|2024-07-03|b7a8a28e69s|\n",
      "| 19| blue|truck|   3| 25.38518620522088| 83|    1|2024-07-02|mz48ksni1n8|\n",
      "+---+-----+-----+----+------------------+---+-----+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def random_date(start, end):\n",
    "    return start + timedelta(days=random.randint(0, int((end - start).days)))\n",
    "\n",
    "def random_geohash():\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=11))\n",
    "\n",
    "data = []\n",
    "start_date = datetime(2024, 7, 1)\n",
    "end_date = datetime(2024, 7, 4)\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "types = [\"SUV\", \"sedan\", \"truck\"]\n",
    "\n",
    "for _ in range(1000):\n",
    "    id = _\n",
    "    color = random.choice(colors)\n",
    "    type = random.choice(types)\n",
    "    hour = random.randint(0, 4)\n",
    "    milesperhour = random.uniform(19.0, 35.0)\n",
    "    age = random.randint(40, 85)\n",
    "\n",
    "    if random.randint(1, 10) < 7:\n",
    "        if color == \"red\":\n",
    "            label = 1\n",
    "        elif color == \"blue\":\n",
    "            label = 2\n",
    "        else:\n",
    "            label = 3\n",
    "    else:\n",
    "        label = random.randint(1, 3)\n",
    "    \n",
    "    date = random_date(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
    "    geohash = random_geohash()\n",
    "    data.append((id, color, type, hour, milesperhour, age, label, date, geohash))\n",
    "\n",
    "schema = [\"id\", \"color\", \"type\", \"hour\", \"milesperhour\", \"age\", \"label\", \"date\", \"geohash\"]\n",
    "data = spark.createDataFrame(data, schema)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5173f746-1d1b-40f6-93ca-593da53750a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string label\n",
    "# data = spark.createDataFrame([\n",
    "#     (0, \"red\",   \"SUV\",   12, 20.0, 60, \"class1\", \"2024-07-01\", \"u4pruydqqvj\"),\n",
    "#     (1, \"red\",   \"sedan\",  9, 30.0, 70, \"class2\", \"2024-07-02\", \"u4pruydqqvk\"),\n",
    "#     (2, \"red\",   \"truck\", 15, 25.0, 80, \"class3\", \"2024-07-01\", \"u4pruydqqvj\"),\n",
    "#     (3, \"blue\",  \"SUV\",   20, 22.0, 65, \"class1\", \"2024-07-02\", \"u4pruydqqvk\"),\n",
    "#     (4, \"blue\",  \"sedan\",  5, 35.0, 75, \"class1\", \"2024-07-01\", \"u4pruydqqvj\"),\n",
    "#     (5, \"blue\",  \"truck\", 12, 28.0, 85, \"class3\", \"2024-07-02\", \"u4pruydqqvk\"),\n",
    "#     (6, \"green\", \"SUV\",    9, 19.0, 50, \"class2\", \"2024-07-03\", \"u4pruydqqvl\"),\n",
    "#     (7, \"green\", \"sedan\", 15, 32.0, 60, \"class3\", \"2024-07-03\", \"u4pruydqqvm\"),\n",
    "#     (8, \"green\", \"truck\", 20, 27.0, 40, \"class1\", \"2024-07-04\", \"u4pruydqqvn\"),\n",
    "#     (9, \"green\", \"SUV\",    5, 21.0, 55, \"class2\", \"2024-07-04\", \"u4pruydqqvo\")\n",
    "# ], [\"id\", \"color\", \"type\", \"hour\", \"milesperhour\", \"age\", \"label\", \"date\", \"geohash\"])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# String Indexing for features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"label\", outputCol=\"label_index\"), # !!!\n",
    "    StringIndexer(inputCol=\"color\", outputCol=\"color_index\"),\n",
    "    StringIndexer(inputCol=\"type\", outputCol=\"type_index\"),\n",
    "    StringIndexer(inputCol=\"hour\", outputCol=\"hour_index\")\n",
    "]\n",
    "\n",
    "# One-Hot Encoding for Logistic Regression (not needed for tree-based models)\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\"],\n",
    "    outputCols=[\"color_vec\", \"type_vec\", \"hour_vec\"]\n",
    ")\n",
    "\n",
    "# Assembling Features for Logistic Regression and Naive Bayes\n",
    "assembler_lr_nb = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"type_vec\", \"hour_vec\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "# Assembling Features for Tree-Based Models\n",
    "assembler_tree = VectorAssembler(\n",
    "    inputCols=[\"color_index\", \"type_index\", \"hour_index\", \"milesperhour\", \"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# MinMaxScaler for Logistic Regression and Naive Bayes\n",
    "scaler_lr_nb = MinMaxScaler(inputCol=\"assembled_features\", outputCol=\"features\")\n",
    "\n",
    "# Create and Fit the Pipeline for Logistic Regression and Naive Bayes\n",
    "pipeline_lr_nb = Pipeline(stages=indexers + [encoder, assembler_lr_nb, scaler_lr_nb])\n",
    "model_lr_nb = pipeline_lr_nb.fit(train_data)\n",
    "transformed_train_data_lr_nb = model_lr_nb.transform(train_data)\n",
    "transformed_test_data_lr_nb = model_lr_nb.transform(test_data)\n",
    "\n",
    "# Create and Fit the Pipeline for Tree-Based Models\n",
    "pipeline_tree = Pipeline(stages=indexers + [assembler_tree])\n",
    "model_tree = pipeline_tree.fit(train_data)\n",
    "transformed_train_data_tree = model_tree.transform(train_data)\n",
    "transformed_test_data_tree = model_tree.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e53b278-1f3a-4554-b6a3-003912e241af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'green', 'red']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_nb.stages[0].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2020b2e6-e9cc-472b-aeed-7d479545214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    if 'assembled_features' in df.schema.names:\n",
    "        feature_attrs = df.schema['assembled_features'].metadata['ml_attr']['attrs']\n",
    "    else:\n",
    "        feature_attrs = df.schema['features'].metadata['ml_attr']['attrs']\n",
    "\n",
    "    features = []\n",
    "    for attr_type, attrs in feature_attrs.items():\n",
    "        features += attrs\n",
    "\n",
    "    # for each in sorted(features, key=lambda x: x['idx']):\n",
    "    #     print(each['idx'], each['name'])\n",
    "    \n",
    "    feature_names = [each['name'] for each in sorted(features, key=lambda x: x['idx'])]\n",
    "\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e4f377a-b2af-4af0-aa2f-b82a41f7138c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['color_vec_blue',\n",
       " 'color_vec_green',\n",
       " 'type_vec_sedan',\n",
       " 'type_vec_truck',\n",
       " 'hour_vec_1',\n",
       " 'hour_vec_0',\n",
       " 'hour_vec_4',\n",
       " 'hour_vec_3',\n",
       " 'milesperhour',\n",
       " 'age']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features(transformed_train_data_lr_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cea68b9-5d45-42a6-9931-da283100d030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['color_index', 'type_index', 'hour_index', 'milesperhour', 'age']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features(transformed_train_data_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f9b1f-3d54-41f2-b6e6-71b445b3cfee",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e733e9-f6b2-42ef-a2fd-8cf836ae7fb3",
   "metadata": {},
   "source": [
    "## save and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dcc9b5e-5e05-4b19-8041-8c41437ed57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save to local\n",
    "data_path = \"data/transformed_train_data_lr_nb/\"\n",
    "\n",
    "# save to s3\n",
    "# data_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/data/transformed_train_data_lr_nb/\"\n",
    "\n",
    "transformed_train_data_lr_nb.write.partitionBy(\"date\", \"geohash\").mode('overwrite').save(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0defbede-c20a-4e91-8b71-7f245bbec36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>color</th>\n",
       "      <th>type</th>\n",
       "      <th>hour</th>\n",
       "      <th>milesperhour</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "      <th>color_index</th>\n",
       "      <th>type_index</th>\n",
       "      <th>hour_index</th>\n",
       "      <th>color_vec</th>\n",
       "      <th>type_vec</th>\n",
       "      <th>hour_vec</th>\n",
       "      <th>assembled_features</th>\n",
       "      <th>features</th>\n",
       "      <th>date</th>\n",
       "      <th>geohash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>786</td>\n",
       "      <td>green</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1</td>\n",
       "      <td>23.314242</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>(1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 23.31...</td>\n",
       "      <td>(0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.269...</td>\n",
       "      <td>2024-07-02</td>\n",
       "      <td>j2jpaekvi54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  color   type  hour  milesperhour  age  label  color_index  type_index  \\\n",
       "0  786  green  sedan     1     23.314242   83      3          1.0         0.0   \n",
       "\n",
       "   hour_index   color_vec    type_vec              hour_vec  \\\n",
       "0         0.0  (0.0, 1.0)  (1.0, 0.0)  (1.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                  assembled_features  \\\n",
       "0  (0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 23.31...   \n",
       "\n",
       "                                            features        date      geohash  \n",
       "0  (0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.269...  2024-07-02  j2jpaekvi54  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "transformed_train_data_lr_nb_loaded = spark.read.load(data_path)\n",
    "\n",
    "# print\n",
    "transformed_train_data_lr_nb_loaded.toPandas().head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02529b-a62d-4de7-9a02-2c511e1ffdd8",
   "metadata": {},
   "source": [
    "## save and load pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e7a7b8f-1ecf-40b7-880c-f9a1bcf7575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "pipeline_model_path = \"pipelines/model_lr_nb\"\n",
    "\n",
    "# save to s3\n",
    "# pipeline_model_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/pipelines/model_lr_nb\"\n",
    "\n",
    "model_lr_nb.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84677c0-d535-40ca-8aab-73a7ea731c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline\n",
    "loaded_model = PipelineModel.load(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb015b-97f1-4b01-b0f2-e1395b57c35f",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f3531c-cca9-4321-8c0b-f00cb5793f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/20 02:27:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "# Training the Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_index\") # !!!\n",
    "lr_model = lr.fit(transformed_train_data_lr_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161044be-e346-403b-889e-ce836d29aecb",
   "metadata": {},
   "source": [
    "## save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d80ceff-e90e-4263-9c44-7805c0a4531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "model_path = \"models/lr_model/\"\n",
    "\n",
    "# save to s3\n",
    "# model_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/models/lr_model/\"\n",
    "\n",
    "lr_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcc353ed-f4c7-433a-80ea-3284ff168adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "lr_model = LogisticRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a545d8-5d6e-4e84-86bc-d7c52086c2d3",
   "metadata": {},
   "source": [
    "## interpret model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "465b0e56-a5aa-415b-af35-072c197945d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: DenseMatrix([[ 0.09863039,  0.6373956 ,  0.04769662, -0.13441972, -0.1233593 ,\n",
      "               0.05768763, -0.16004556,  0.16768973,  0.03963142,  0.08391418],\n",
      "             [-1.54615485, -1.67036284, -0.05615882, -0.07673807,  0.06647786,\n",
      "               0.1301485 ,  0.22368808, -0.1703098 , -0.15508396,  0.13825928],\n",
      "             [ 1.64706143, -0.5214493 ,  0.03806322,  0.05313225, -0.18434929,\n",
      "              -0.20192988, -0.24674252, -0.31196386,  0.17187505,  0.07875167],\n",
      "             [-0.19953696,  1.55441653, -0.02960102,  0.15802554,  0.24123073,\n",
      "               0.01409376,  0.18309999,  0.31458394, -0.05642252, -0.30092513]])\n",
      "Intercept: [-16.472841777995686,6.569559211345696,5.002800057733383,4.900482508916609]\n"
     ]
    }
   ],
   "source": [
    "# Get model coefficients and intercept for Logistic Regression\n",
    "coefficients = lr_model.coefficientMatrix\n",
    "intercept = lr_model.interceptVector\n",
    "print(f\"Coefficients: {coefficients}\")\n",
    "print(f\"Intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51b2793-848e-4dd7-afcc-62669c17caa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape = num_classes x num_features\n",
    "np.array(coefficients.toArray().tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f338b574-650f-4bcf-96ac-d244eede69dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (3) does not match length of index (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m coef_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(coefficients\u001b[38;5;241m.\u001b[39mtoArray()\u001b[38;5;241m.\u001b[39mtolist()),\n\u001b[1;32m      3\u001b[0m     columns\u001b[38;5;241m=\u001b[39mget_features(transformed_train_data_lr_nb)\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# map label_index to label\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcoef_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mstages[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlabels\n\u001b[1;32m      8\u001b[0m coef_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(coef_df\u001b[38;5;241m.\u001b[39mcolumns)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m~/ghq/github.com/mathmathpp2/synapselightgbm_trial/synapse-test/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/mathmathpp2/synapselightgbm_trial/synapse-test/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4172\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4174\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4177\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4178\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4179\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4180\u001b[0m     ):\n\u001b[1;32m   4181\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/ghq/github.com/mathmathpp2/synapselightgbm_trial/synapse-test/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4915\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ghq/github.com/mathmathpp2/synapselightgbm_trial/synapse-test/.venv/lib/python3.11/site-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (3) does not match length of index (4)"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame(\n",
    "    np.array(coefficients.toArray().tolist()),\n",
    "    columns=get_features(transformed_train_data_lr_nb)\n",
    ")\n",
    "\n",
    "# map label_index to label\n",
    "coef_df['label'] = loaded_model.stages[0].labels\n",
    "coef_df[['label'] + list(coef_df.columns)[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55fe0b2-484e-41ed-922e-4cad1405d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df2 = coef_df.reset_index().melt(id_vars=['label'], var_name='feature', value_name='coefficient')\n",
    "coef_df2[['label', 'feature', 'coefficient']].sort_values(by=['label', 'feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b702a-b016-4f92-94ce-ce66692df134",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ceda200-66c7-482f-bf79-db7c00b1ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize evaluators for all models\n",
    "# evaluator_accuracy = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"accuracy\")\n",
    "# evaluator_precision = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"weightedPrecision\")\n",
    "# evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"weightedRecall\")\n",
    "# evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label_index\", metricName=\"f1\")\n",
    "\n",
    "# Initialize evaluators for all models\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e6daa0c-5743-486a-9ddd-49cea37f9195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7014218009478673\n",
      "Logistic Regression Precision: 0.7123299902820629\n",
      "Logistic Regression Recall: 0.7014218009478672\n",
      "Logistic Regression F1 Score: 0.702552651112263\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Logistic Regression Model\n",
    "lr_predictions = lr_model.transform(transformed_test_data_lr_nb)\n",
    "lr_accuracy = evaluator_accuracy.evaluate(lr_predictions)\n",
    "lr_precision = evaluator_precision.evaluate(lr_predictions)\n",
    "lr_recall = evaluator_recall.evaluate(lr_predictions)\n",
    "lr_f1 = evaluator_f1.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
    "print(f\"Logistic Regression Precision: {lr_precision}\")\n",
    "print(f\"Logistic Regression Recall: {lr_recall}\")\n",
    "print(f\"Logistic Regression F1 Score: {lr_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef4f13-b7e8-4170-b9d1-b003c1db594a",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adcc3c8-6f05-41e8-8ad8-145e5b75b3db",
   "metadata": {},
   "source": [
    "## save and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddf93b27-fe3e-46d4-9319-911a4f23554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "data_path = \"data/transformed_train_data_tree/\"\n",
    "\n",
    "# save to s3\n",
    "# data_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/data/transformed_train_data_tree/\"\n",
    "\n",
    "transformed_train_data_tree.write.partitionBy(\"date\", \"geohash\").mode('overwrite').save(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "822d8429-db42-4e16-9ea6-24746c84d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "transformed_train_data_tree_loaded = spark.read.load(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1748c-58c8-407c-be71-27e7898889ae",
   "metadata": {},
   "source": [
    "## save and load pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4c16263-cd80-4130-b758-fc9c2e026e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "pipeline_model_path = \"pipelines/model_tree\"\n",
    "\n",
    "# save to s3\n",
    "# pipeline_model_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/pipelines/model_tree\"\n",
    "\n",
    "model_tree.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa44a5e6-2e5b-4ad5-8084-22eb26a24d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline\n",
    "loaded_model = PipelineModel.load(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a441659-18ae-4a43-a0ce-c68dc2c9d5df",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "04d809de-ac10-4f03-9f1b-12945e87bf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/19 10:07:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 9 (= number of training instances)\n"
     ]
    }
   ],
   "source": [
    "# Training the Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label_index\")\n",
    "rf_model = rf.fit(transformed_train_data_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ff62b-afca-4de5-a86d-1cb63ef92f4e",
   "metadata": {},
   "source": [
    "## save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7dcc396e-3531-45b9-8f18-69803aa7f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "model_path = \"models/rf_model/\"\n",
    "\n",
    "# save to s3\n",
    "# model_path = \"s3a://test-thama-misc-20210612/20240717-sparkml/models/rf_model/\"\n",
    "\n",
    "rf_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5fb66297-ccbf-4048-8ac3-9fd75d9d7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "rf_model = RandomForestClassificationModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6d98b-ff38-470c-abbe-a69f19debc82",
   "metadata": {},
   "source": [
    "## interpret model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d787dddf-49b5-472d-ad56-b09920cd23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances for Random Forest\n",
    "rf_feature_importances = rf_model.featureImportances.toArray()\n",
    "features_importances_rf = [(assembler_tree.getInputCols()[i], float(rf_feature_importances[i])) for i in range(len(rf_feature_importances))]\n",
    "importances_df_rf = pd.DataFrame(features_importances_rf, columns=[\"Feature\", \"Importance\"]).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a38850c-d279-4abe-b2e9-2ebb8cf70227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>color_index</td>\n",
       "      <td>0.301543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milesperhour</td>\n",
       "      <td>0.298617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hour_index</td>\n",
       "      <td>0.258801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type_index</td>\n",
       "      <td>0.075918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>age</td>\n",
       "      <td>0.065121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Importance\n",
       "0   color_index    0.301543\n",
       "3  milesperhour    0.298617\n",
       "2    hour_index    0.258801\n",
       "1    type_index    0.075918\n",
       "4           age    0.065121"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_df_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6e3ac-b053-4ed3-8f5b-6e4d1968c9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
